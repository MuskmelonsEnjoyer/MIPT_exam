# Экзамен

Датасет  Breast Cancer Wisconsin Diagnostic содержи в себе информацию о злокачественных и доброкачественных опухолей молочной железы.

Для анализа данных была использована библиоетка pandas.
Функции describe и info использованы для первичного анализа, так можно обнаружить наличие пропусков, выбросов, узнать тип данных.

Для формирования датасета использовались значения диагнозов, которые представленны в виде категориальных признаков, где M - злокачественные образцы, B - доброкачесвенные.

В рамках датасета решается классическая задача бинарной классификации, где на основе признаков опухоли модель должна выдать предсказание является опухоль злакачественной, либо доброкачественной.

Конечный датасет состоит из 30 полей, которые содержат численные признаки об опухоли и кодированные признаки для классификации.

Для решения задачи была использована логистическая модель.

Архитектура модели:
Скрипт dataset.py загружает через url ссылку датасет и проводит первичную предобработку данных, после чего в скрипт model.py импортируются признаки, которые поступают на вход модели для её последущего обучения и сохранения.

В конечном итоге метрики модели таковы:

Accuracy на тесте 0.95
recall на тесте: 0.99
precision на тесте: 0.93
F1 на тесте 0.96

DAG pipeline_dag состоит из четырёх взаимосвязанных задач, которые выполняются последовательно:
extract → 2. train_model → 3. evaluate_model → 4. upload_to_s3
1. extract: загружает CSV с данными WDBC и сохраняет во временный файл /tmp/data_wdbc.csv.
2. train_model: читает /tmp/data_wdbc.csv, обучает логистическую регрессию и сохраняет модель в /tmp/model.pkl.
3. evaluate_model: оценивает обученную модель на тестовой выборке (recall, precission, f1) и пишет результаты в логи.
4. upload_to_s3: выгружает файлы /tmp/data_wdbc.csv и /tmp/model.pkl в указанный S3-бакет с ключами, содержащими дату.

Доступные команды:

airflow dags list # Список DAG
airflow dags trigger pipeline_dag # Триггер DAG
airflow tasks test <dag_id> <task_id> <run_date> # Тестирование задачи
less $AIRFLOW_HOME/logs/pipeline_dag/extract/2025-06-18T00:00:00+00:00/1.log # Просмотр логов задачи

Веса обученной модели хронятся локально в папке /results.
Поскольку в sklearn не реализован метод сохранения весов используется библиотека joblib с удобным интерфейсом сохранения и загрузки весов. Также совместимость с sklearn и скорость выполнения являются преимуществами, из-за которых была выбрана эта библиотека.

В данном проекте реализация хранилища осуществляется через AWS S3.


