# Экзамен

# Формирование датасета 
Датасет  Breast Cancer Wisconsin Diagnostic содержи в себе информацию о злокачественных и доброкачественных опухолях молочной железы.

Для анализа данных была использована библиотека pandas.
Функции describe и info использованы для первичного анализа, так можно обнаружить наличие пропусков, выбросов, узнать тип данных.

Для формирования датасета использовались значения диагнозов, которые представлены в виде категориальных признаков, где M - злокачественные образцы, B - доброкачественные.

В рамках датасета решается классическая задача бинарной классификации, где на основе признаков опухоли модель должна выдать предсказание является опухоль злокачественной, либо доброкачественной.

Конечный датасет состоит из 30 полей, которые содержат численные признаки об опухоли и кодированные признаки для классификации.

# Логистическая модель
Для решения задачи была использована логистическая модель.

Архитектура модели:
Скрипт dataset.py загружает через url ссылку датасет и проводит первичную предобработку данных, после чего в скрипт model.py импортируются признаки, которые поступают на вход модели для её последующего обучения и сохранения.

В конечном итоге метрики модели таковы:

Accuracy на тесте 0.95
recall на тесте: 0.99
precision на тесте: 0.93
F1 на тесте 0.96

# Apache Airflow
DAG pipeline_dag состоит из четырёх взаимосвязанных задач, которые выполняются последовательно:
extract → 2. train_model → 3. evaluate_model → 4. upload_to_s3
1. extract: загружает CSV с данными WDBC и сохраняет во временный файл /tmp/data_wdbc.csv.
2. train_model: читает /tmp/data_wdbc.csv, обучает логистическую регрессию и сохраняет модель в /tmp/model.pkl.
3. evaluate_model: оценивает обученную модель на тестовой выборке (recall, precission, f1) и пишет результаты в логи.
4. upload_to_s3: выгружает файлы /tmp/data_wdbc.csv и /tmp/model.pkl в указанный S3-бакет с ключами, содержащими дату.

Доступные команды:

airflow dags list # Список DAG
airflow dags trigger pipeline_dag # Триггер DAG
airflow tasks test <dag_id> <task_id> <run_date> # Тестирование задачи
less $AIRFLOW_HOME/logs/pipeline_dag/extract/2025-06-18T00:00:00+00:00/1.log # Просмотр логов задачи

Веса обученной модели хронятся локально в папке /results.
Поскольку в sklearn не реализован метод сохранения весов используется библиотека joblib с удобным интерфейсом сохранения и загрузки весов. Также совместимость с sklearn и скорость выполнения являются преимуществами, из-за которых была выбрана эта библиотека.

В данном проекте реализация хранилища осуществляется через AWS S3.

# Развитие проекта

Потенциально можно добавить config файл, в который перенести, например url ссылку на элемент.
Использовать docker

# Анализ ошибок

## Где может «упасть» процесс?

1. На этапе extract, если url ссылка будет недействительна;
2. Повреждённый csv;
3. Недостаточно памяти на локальном диске;
4. На этапе training не хватило оперативной памяти

## Какие исключения могут возникнуть?

В основном Timeout, HTTPError и другие, связанные с подключением к сети.

## Что произойдет при потере соединения с источником данных?
Поскольку установлен retry_delay, будет произведена попытка повторного подключение через 5 минут.

## Что будет, если источник отдает невалидные данные?

В зависимости от природы невалидных данных могут быть различные сценарии:
1. Упадет таска extract
2. Пройдёт до модели, но выдаст ошибку в этой таске.

Поскольку выбран 'depends_on_past': False, то в случае ошибки остальные таски получат статус пропущенных.

Для отслеживания ошибок предусмотрено логирование у каждой таски.
